# --------------------------
# Tab 2: Analytics
# --------------------------
def analytics_tab(model, model_metadata, dataset_info):
    """Complete model analytics on all images"""
    fruit_names = model_metadata['fruit_names']
    image_paths = dataset_info['image_paths']
    labels = dataset_info['labels']
    
    st.header("ðŸ“Š Complete Model Analytics")
    st.markdown("Comprehensive analysis of model performance on all images")
    
    # Initialize session state
    if 'analytics_complete' not in st.session_state:
        st.session_state.analytics_complete = False
    
    # Show appropriate button based on state
    if not st.session_state.analytics_complete:
        # Not analyzed yet - show info and button
        st.info(f"ðŸ“Š Click the button below to analyze all {len(image_paths)} images")
        st.warning("â±ï¸ This may take a few minutes depending on dataset size")
        
        if st.button("ðŸ” Run Full Analysis", type="primary", use_container_width=True):
            with st.spinner("Analyzing all images... Please wait..."):
                predictions, true_labels, confidences, misclassified = analyze_all_images(
                    model, image_paths, labels, fruit_names
                )
                
                # Store in session state
                st.session_state.predictions = predictions
                st.session_state.true_labels = true_labels
                st.session_state.confidences = confidences
                st.session_state.misclassified = misclassified
                st.session_state.analytics_complete = True
                st.rerun()
    
    else:
        # Analysis complete - show results and option to rerun
        col1, col2 = st.columns([4, 1])
        with col1:
            st.success("âœ… Analysis complete! Results shown below.")
        with col2:
            if st.button("ðŸ”„ Rerun Analysis", type="secondary"):
                st.session_state.analytics_complete = False
                st.rerun()
        
        predictions = st.session_state.predictions
        true_labels = st.session_state.true_labels
        confidences = st.session_state.confidences
        misclassified = st.session_state.misclassified
        
        # Overall metrics
        st.markdown("---")
        st.subheader("ðŸŽ¯ Overall Performance")
        
        overall_acc = accuracy_score(true_labels, predictions)
        avg_confidence = np.mean(confidences)
        correct_count = sum(1 for t, p in zip(true_labels, predictions) if t == p)
        incorrect_count = len(true_labels) - correct_count
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Overall Accuracy", f"{overall_acc*100:.2f}%")
        with col2:
            st.metric("Avg Confidence", f"{avg_confidence*100:.2f}%")
        with col3:
            st.metric("Correct", correct_count)
        with col4:
            st.metric("Incorrect", incorrect_count)
        
        # Confusion Matrix
        st.markdown("---")
        st.subheader("ðŸ”¢ Confusion Matrix")
        
        cm = confusion_matrix(true_labels, predictions)
        
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=fruit_names, yticklabels=fruit_names,
                    ax=ax, cbar_kws={'label': 'Count'})
        ax.set_xlabel('Predicted Label')
        ax.set_ylabel('True Label')
        ax.set_title('Confusion Matrix')
        plt.tight_layout()
        st.pyplot(fig)
        
        # Per-Class Metrics
        st.markdown("---")
        st.subheader("ðŸ“ˆ Per-Class Performance")
        
        report = classification_report(true_labels, predictions, 
                                       target_names=fruit_names, 
                                       output_dict=True)
        
        # Create DataFrame
        per_class_data = []
        for fruit in fruit_names:
            per_class_data.append({
                'Fruit': fruit,
                'Precision': f"{report[fruit]['precision']*100:.2f}%",
                'Recall': f"{report[fruit]['recall']*100:.2f}%",
                'F1-Score': f"{report[fruit]['f1-score']*100:.2f}%",
                'Support': int(report[fruit]['support'])
            })
        
        per_class_df = pd.DataFrame(per_class_data)
        st.dataframe(per_class_df, use_container_width=True)
        
        # Confidence Distribution
        st.markdown("---")
        st.subheader("ðŸ“Š Confidence Distribution")
        
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.hist(confidences, bins=50, edgecolor='black', alpha=0.7)
        ax.set_xlabel('Confidence')
        ax.set_ylabel('Frequency')
        ax.set_title('Distribution of Prediction Confidences')
        ax.axvline(avg_confidence, color='red', linestyle='--', 
                   label=f'Mean: {avg_confidence:.3f}')
        ax.legend()
        plt.tight_layout()
        st.pyplot(fig)
        
        # Training History (if available)
        history = load_training_history()
        if history:
            st.markdown("---")
            st.subheader("ðŸ“‰ Training History")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Loss curve
                fig, ax = plt.subplots(figsize=(8, 5))
                epochs = range(1, len(history['train_loss']) + 1)
                ax.plot(epochs, history['train_loss'], 'b-', label='Training Loss')
                ax.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')
                ax.set_xlabel('Epoch')
                ax.set_ylabel('Loss')
                ax.set_title('Training and Validation Loss')
                ax.legend()
                ax.grid(True, alpha=0.3)
                plt.tight_layout()
                st.pyplot(fig)
            
            with col2:
                # Accuracy curve
                fig, ax = plt.subplots(figsize=(8, 5))
                ax.plot(epochs, [acc*100 for acc in history['train_acc']], 
                       'b-', label='Training Accuracy')
                ax.plot(epochs, [acc*100 for acc in history['val_acc']], 
                       'r-', label='Validation Accuracy')
                ax.set_xlabel('Epoch')
                ax.set_ylabel('Accuracy (%)')
                ax.set_title('Training and Validation Accuracy')
                ax.legend()
                ax.grid(True, alpha=0.3)
                plt.tight_layout()
                st.pyplot(fig)
        
        # Misclassification Analysis
        st.markdown("---")
        st.subheader("âŒ Misclassification Analysis")
        
        st.metric("Total Misclassified", len(misclassified))
        
        if len(misclassified) > 0:
            # Show worst misclassifications (highest confidence but wrong)
            st.markdown("#### Top 10 Most Confident Mistakes")
            
            sorted_mistakes = sorted(misclassified, 
                                    key=lambda x: x['confidence'], 
                                    reverse=True)[:10]
            
            mistake_data = []
            for mistake in sorted_mistakes:
                mistake_data.append({
                    'Image': Path(mistake['image_path']).name,
                    'True Label': mistake['true_label'],
                    'Predicted': mistake['predicted'],
                    'Confidence': f"{mistake['confidence']*100:.1f}%"
                })
            
            mistake_df = pd.DataFrame(mistake_data)
            st.dataframe(mistake_df, use_container_width=True)
            
            # Show sample misclassifications
            st.markdown("#### Sample Misclassified Images")
            
            num_samples = min(10, len(sorted_mistakes))
            cols = st.columns(5)
            
            for i in range(num_samples):
                col_idx = i % 5
                mistake = sorted_mistakes[i]
                
                with cols[col_idx]:
                    img = Image.open(mistake['image_path']).convert('L')
                    st.image(img, use_container_width=True)
                    st.caption(f"True: {mistake['true_label']}")
                    st.caption(f"Pred: {mistake['predicted']}")
                    st.caption(f"Conf: {mistake['confidence']*100:.1f}%")
